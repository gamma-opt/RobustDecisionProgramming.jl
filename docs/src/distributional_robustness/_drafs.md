# Drafts
For example, we applied robust decision programming to the [N-monitoring](https://gamma-opt.github.io/DecisionProgramming.jl/dev/examples/n-monitoring/) example from decision programming such that we made the failure node robust.

Then, we define the **continuous uncertainty set** that consists of all deviated distributions around the pivot distribution

$$\bar{ğ}_ğ©=\{ğ©+ğâˆ£ğâˆˆ\bar{ğƒ}_ğ©\}.$$

An **uncertainty set** $ğ_{ğ©}$ is a finite subset of $\bar{ğ}_{ğ©}$ such that it contains all minimizing probabilities for the **maximin expected value** objective.

---

We begin by denoting the continuous uncertainty set as

$$\bar{ğ}_ğ©=\{ğ©+ğâˆ£ğâˆˆ\bar{ğƒ}_ğ©\}.$$

We can define the minimum expected value over the continuous uncertainty set as

$$\min_{ğªâˆˆ\bar{ğ}_ğ©} ğ”¼(ğª, ğ®) = \min_{ğâˆˆ\bar{ğƒ}_ğ©} ğ”¼(ğ©+ğ, ğ®) = ğ”¼(ğ©,ğ®) + \min_{ğâˆˆ\bar{ğƒ}_ğ©} ğ”¼(ğ,ğ®).$$

---

We define **cross-assignment** for ordering $u_1â‰¤u_2â‰¤...â‰¤u_k$ as an assignment of deviations to **positive deviations** $d_1,...,d_jâ‰¥0$ and **negative deviations** $d_{j+1},...,d_kâ‰¤0$ where $jâˆˆ\{1,...,k-1\}$ such that they satisfy the constraints. An **optimal cross-assignment** finds values for the positive and negative deviations that minimize the objective.


### Over Uncertainty Set
We define an **uncertainty set** as a finite set of probability vectors $ğ.$ Then, similar to [Wald's maximin model](https://en.wikipedia.org/wiki/Wald%27s_maximin_model), the problem as maximizing the minimum expected value over the uncertainty set over decision variables $Z$ is

$$\max_{zâˆˆZ}\, \min_{ğªâˆˆğ} ğ”¼(ğª, ğ®(z)).$$

We can reformulate the **maxmin over uncertainty set** in mathematical programming as

$$\max_{zâˆˆZ}\, x$$

---

$$xâ‰¤ğ”¼(ğª, ğ®(z)),\quad âˆ€ğªâˆˆğ.$$

Regarding to the computational complexity, the number of constraints in the above formulation is proportional to the total size of the uncertainty sets

$$âˆ‘_{lâˆˆL} |ğ_l|.$$


### Minimax Regret
Solve the original, non-robust problem by maximizing the expected value over decision variables $Z$

$$Î¼^{âˆ—}=\underset{zâˆˆZ}{\operatorname{maximize}}\, ğ”¼(ğ©,ğ®(z)).$$

We formulate the minimization of the maximum regret as

$$\underset{zâˆˆZ}{\operatorname{minimize}}\, \max_{ğªâˆˆğ_ğ©} (Î¼^{âˆ—}-ğ”¼(ğª, ğ®(z)))$$

$$\underset{zâˆˆZ}{\operatorname{minimize}}\, (Î¼^{âˆ—} - \min_{ğªâˆˆğ_ğ©} ğ”¼(ğª, ğ®(z))$$

Next, we linearize the expression to a form

$$\underset{zâˆˆZ}{\operatorname{minimize}}\, (Î¼^{âˆ—}-x)$$

$$xâ‰¤ğ”¼(ğª, ğ®(z)),\quad âˆ€ğªâˆˆğ_ğ©$$


### Minimax Regret
Solve the original, non-robust problem by maximizing the expected value over decision variables $Z$

$$Î¼^{âˆ—}=\underset{zâˆˆZ}{\operatorname{maximize}}\, âˆ‘_{l=1}^m ğ”¼(ğ©_l, ğ®_l(z))$$

We formulate the minimization of the maximum regret as

$$\underset{zâˆˆZ}{\operatorname{minimize}}\, \max_{(ğª_1,...,ğª_l)âˆˆğ_ğ^{Ã—}} (Î¼^{âˆ—}-âˆ‘_{l=1}^m ğ”¼(ğª_l, ğ®_l(z)))$$

$$\underset{zâˆˆZ}{\operatorname{minimize}}\, (Î¼^{âˆ—} - \min_{(ğª_1,...,ğª_l)âˆˆğ_ğ^{Ã—}} âˆ‘_{l=1}^m ğ”¼(ğª_l, ğ®_l(z)))$$

Next, we linearize the expression to a form

$$\underset{zâˆˆZ}{\operatorname{minimize}}\, (Î¼^{âˆ—} - âˆ‘_{l=1}^m x_l)$$

$$x_l â‰¤ ğ”¼(ğª, ğ®_l(z)),\quad âˆ€ğªâˆˆğ_{ğ©_l},\, lâˆˆ\{1,...,m\}$$


## Proof of Negativity
The objective value of cross-assignment is always negative or zero.

We define **cross-assignment** for ordering $u_1â‰¤u_2â‰¤...â‰¤u_k$ as an assignment of deviations to **positive deviations** $d_1,...,d_jâ‰¥0$ and **negative deviations** $d_{j+1},...,d_kâ‰¤0$ where $jâˆˆ\{1,...,k-1\}$ such that they satisfy the constraints.

---

For a cross-assignment with $k=2$ and $j=1$ we have:

$$\begin{aligned}
u_1â‹…d_1 + u_2â‹…d_2 &â‰¤ 0 \\
u_1â‹…d_1 &â‰¤ u_2â‹…(-d_2) \\
u_1â‹…d_1 &â‰¤ u_2â‹…d_1 \\
u_1 &â‰¤ u_2.
\end{aligned}$$

---

For cross-assignment with $k>2$ and for all $jâˆˆ\{1,...,k-1\}$ we have:

$$\begin{aligned}
u_1â‹…d_1 + ... + u_kâ‹…d_k &â‰¤ u_jâ‹…d_1 + ... + u_jâ‹…d_j + u_{j+1}â‹…d_{j+1} + ... + u_{j+1}â‹…d_{k} \\
&= u_jâ‹…(d_1+...+d_j) + u_{j+1}â‹…(d_{j+1}+...+d_k) \\
& â‰¤ 0.
\end{aligned}$$

We obtain the last step from the result for $k=2.$


## Proof of Minimum
The condition that some cross-assignment is less or equal to another cross-assignment.

Let $u_1â‰¤u_2$ and $d_1+d_2=d_1^{â€²}+d_2^{â€²}$ where $d_1=d_1^{â€²}+d^{â€²â€²}$ and $d_2=d_2^{â€²}-d^{â€²â€²}$ with $d^{â€²â€²}â‰¥0.$ Then, we have:

$$\begin{aligned}
u_1â‹…d_1+u_2â‹…d_2 &= u_1â‹…(d_1^{â€²}+d^{â€²â€²})+u_2â‹…d_2 \\
&= u_1â‹…d_1^{â€²}+u_1â‹…d^{â€²â€²}+u_2â‹…d_2 \\
&â‰¤ u_1â‹…d_1^{â€²}+u_2â‹…d^{â€²â€²}+u_2â‹…d_2 \\
&= u_1â‹…d_1^{â€²}+u_2â‹…(d_2+d^{â€²â€²}) \\
&= u_1â‹…d_1^{â€²}+u_2â‹…d_2^{â€²}.
\end{aligned}$$

It satisfies the constraint

$$|d_1|+|d_2|=|d_1^{â€²}+d^{â€²â€²}|+|d_2^{â€²}-d^{â€²â€²}|=|d_1^{â€²}|+|d_2^{â€²}|$$

2) Smallest $l$: $d_1,d_1^{â€²},d_2,d_2^{â€²}â‰¥0$
3) Largest $h$: $d_1,d_1^{â€²},d_2,d_2^{â€²}â‰¤0$
1) Largest $Î´$: $d_1,d_1^{â€²}â‰¥0 âˆ§ d_2,d_2^{â€²}â‰¤0$

---

Given an initial cross-assignment $ğ=(d_1,d_2,...,d_k),$ the recursive step updates it to a new cross-assignment $ğ_{l,h,Î´}^{â€²}=(d_1^{â€²},d_2^{â€²},...,d_k^{â€²})$ such that $d_l^{â€²}=d_l+Î´$, $d_h^{â€²}=d_h-Î´$ and $d_i^{â€²}=d_i, âˆ€iâˆˆIâˆ–\{l,h\}$ with $lâˆˆI$ and $hâˆˆI$ and $Î´â‰¥0.$

-  $ğ_{l,h,Î´}^{â€²}â‹…ğ®â‰¤ğâ‹…ğ®$ if $lâ‰¤h$ and $Î´â‰¥0$
-  $ğ_{l_1,h_1,Î´_1}^{â€²}â‹…ğ®â‰¤ğ_{l_2,h_2,Î´_2}â‹…ğ®$ if
    -  $l_1â‰¤l_2$ and $Î´_1=Î´_2$ or
    -  $h_1â‰¥h_2$ and $Î´_1=Î´_2$ or
    -  $Î´_1â‰¥Î´_2$ where $l_1=l_2$ and $h_1=h_2$
