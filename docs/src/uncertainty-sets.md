# Uncertainty Sets
## Continuous Uncertainty Set
Given two finite sets of discrete probabilities, the **pivot** $ğ©$ and **deviated** $ğª$ over states $I.$ We define the **difference** between the distributions as

$$ğ=ğª-ğ©.$$

Alternatively, we can describe the deviated distribution as the sum of pivot distribution and difference

$$ğª=ğ©+ğ.$$

Then, we define the **continuous uncertainty set** that consists of all deviated distributions around the pivot distribution

$$\bar{ğ}_ğ©=\{ğ©+ğâˆ£ğâˆˆğƒ_ğ©\},$$

where $ğƒ_ğ©$ is an ambiguity set for the pivot $ğ©.$


## Ambiguity Set
From the properties of discrete probabilities, we have the lower and upper bounds

$$0â‰¤ğªâ‰¤1$$

$$0â‰¤ğ©+ğâ‰¤1$$

$$-ğ©â‰¤ğâ‰¤1-ğ©.$$

We define the **lower bounds** $ğ^{-}$ and **upper bounds** $ğ^{+}$ as parameters such that $ğ^{-}â‰¤ğâ‰¤ğ^{+}$ where $-ğ©â‰¤ğ^{-}â‰¤0$ and $0â‰¤ğ^{+}â‰¤1-ğ©.$ These bound also quarantee that $ğ©$ belongs to the ambiguity set $ğƒ_ğ©.$

---

As a consequence of the properties of discrete probabilities, the sum of the differences is zero

$$ğâ‹…ğŸ(k)=(ğ©-ğª)â‹…ğŸ(k)=ğ©â‹…ğŸ(k)-ğªâ‹…ğŸ(k)=0.$$

---

Additionally, we can limit the magnitude of the differences with $lâˆˆâ„•$ norm

$$\|ğ\|_lâ‰¤2Ïµ,$$

with a **radius** parameter $0â‰¤Ïµâ‰¤1.$

---

The **ambiguity set** is the set of all difference vectors that satisfy the given conditions

$$ğƒ_ğ©=\{ğâˆˆ[ğ^{-},ğ^{+}]âˆ£ ğâ‹…ğŸ(k)=0,\, \|ğ\|_lâ‰¤2Ïµ\}.$$

The ambiguity set is convex, which makes optimization possible.

---

Proof of convexity: Let $ğ_1,ğ_2âˆˆğƒ_ğ©,$ we must show that $ğâˆˆğƒ_ğ©$ where $ğ=(1-Î»)ğ_1+Î»ğ_2$ with $Î»âˆˆ[0,1].$

1) $ğ=(1-Î»)ğ_1+Î»ğ_2â‰¥(1-Î»)ğ^{-}+Î»ğ^{-}=ğ^{-}.$
2) $ğ=(1-Î»)ğ_1+Î»ğ_2â‰¤(1-Î»)ğ^{+}+Î»ğ^{+}=ğ^{+}.$
3) $ğâ‹…ğŸ(k)=(1-Î»)ğ_1â‹…ğŸ(k)+Î»ğ_2â‹…ğŸ(k)=0$
4) $\|ğ\|_lâ‰¤(1-Î»)\|ğ_1\|_l+Î»\|ğ_2\|_lâ‰¤2Ïµ$ (Triangle inequality)

---

Decreasing $l$ makes the model more pessimistic. Using $l=1$ we receive a **polyhedral ambiguity set**. By setting $Ïµ=1$ we can make the magnitude constraint inactive.


## Discrete Uncertainty Set
We cannot use a continuous uncertainty set directly for formulating the mathematical model. We must obtain a discrete subset of the continuous uncertainty set to linearize the minimum expected value in the [Best Worst-Case Expected Value](@ref) page.

We can define the minimum expected value over the continuous uncertainty set with utilities $ğ®$ as

$$\min_{ğªâˆˆ\bar{ğ}_ğ©} ğ”¼(ğª, ğ®) = \min_{ğâˆˆğƒ_ğ©} (ğ©+ğ)â‹…ğ® = ğ©â‹…ğ® + \min_{ğâˆˆğƒ_ğ©} ğâ‹…ğ®.$$

We can express the minimizing difference as

$$ğ^{âˆ—}(ğ®)=\argmin_{ğâˆˆğƒ_ğ©} ğâ‹…ğ®.$$

Now, we can obtain a discrete ambiguity set

$$Î”_ğ©=\{ğ^{âˆ—}(ğ®)âˆ£ğ®âˆˆâ„^k\}.$$

The discrete set of all possible minimizing distributions

$$ğ_ğ©=\{ğ©+ğâˆ£ğâˆˆÎ”_ğ©\}.$$


## Discrete Polyhedral Uncertainty Set
### Cross-Assignment
The minimization problem over a polyhedral ambiguity set $(l=1)$ is

$$\argmin_{(d_1,...,d_k)âˆˆâ„^k} \, d_1â‹…u_1 +d_2â‹…u_2 +...+d_kâ‹…u_k$$

$$d_i^{-} â‰¤ d_i â‰¤ d_i^{+}, \quad âˆ€iâˆˆ\{1,...,k\}$$

$$d_1+d_2+...+d_k=0$$

$$\|ğ\|_1=|d_1|+|d_2|+...+|d_k|â‰¤2Ïµ.$$

The parameters are lower bound $d_i^{-}âˆˆ[-p_i,0]$ and upper bound $d_i^{+}âˆˆ[0,1-p_i]$ for all $iâˆˆ\{1,...,k\},$ the radius parameter is $Ïµâˆˆ[0,1]$ and an ordering for the utilities $ğ®=(u_1,...,u_k).$ We will use the ordering

$$u_1â‰¤u_2â‰¤...â‰¤u_k.$$

We define **cross-assignment** as assignment of differences to **positive differences** $d_1,...,d_jâ‰¥0$ and **negative differences** $d_{j+1},...,d_kâ‰¤0$ where $jâˆˆ\{1,...,k-1\}$ that satisfies the constraints.

An **optimal cross-assignment** minimizes the objective.

### Proofs
For $k=2$ we have $u_1â‰¤u_2$ and $d_1+d_2=0$ where $d_1â‰¥0$ and $d_2â‰¤0$

$$\begin{aligned}
u_1â‹…d_1 + u_2â‹…d_2 &â‰¤ 0 \\
u_1â‹…d_1 &â‰¤ u_2â‹…(-d_2) \\
u_1â‹…d_1 &â‰¤ u_2â‹…d_1 \\
u_1 &â‰¤ u_2.
\end{aligned}$$

---

If $kâ‰¥2$:

$$\begin{aligned}
u_1â‹…d_1 + ... + u_kâ‹…d_k &â‰¤ u_jâ‹…d_1 + ... + u_jâ‹…d_j + u_{j+1}â‹…d_{j+1} + ... + u_{j+1}â‹…d_{k} \\
&= u_jâ‹…(d_1+...+d_j) + u_{j+1}â‹…(d_{j+1}+...+d_k) \\
& â‰¤ 0.
\end{aligned}$$

We obtain the last step from binary cross-assignment.

### Minimizing cross-assignment
Let $u_1â‰¤u_2$ and $d_1+d_2=d_1^{â€²}+d_2^{â€²}.$ Then

$$u_1â‹…d_1+u_2â‹…d_2â‰¤u_1â‹…d_1^{â€²}+u_2â‹…d_2^{â€²}$$

---

Let $d_1,d_2,d_1^{â€²},d_2^{â€²},d^{â€²â€²}â‰¤0.$ Then

$$\begin{aligned}
u_1â‹…d_1+u_2â‹…d_2 &= u_1â‹…d_1+u_2â‹…(d_2^{â€²}+d^{â€²â€²}) \\
&= u_1â‹…d_1+u_2â‹…d^{â€²â€²}+u_2â‹…d_2^{â€²} \\
&â‰¤ u_1â‹…d_1+u_1â‹…d^{â€²â€²}+u_2â‹…d_2^{â€²} \\
&= u_1â‹…(d_1+d^{â€²â€²})+u_2â‹…d_2^{â€²} \\
&= u_1â‹…d_1^{â€²}+u_2â‹…d_2^{â€²},
\end{aligned}$$

where $d_1=d_1^{â€²}-d^{â€²â€²}$ and $d_2=d_2^{â€²}+d^{â€²â€²}.$

---

Let $d_1,d_2,d_1^{â€²},d_2^{â€²},d^{â€²â€²}â‰¥0.$ Then

$$\begin{aligned}
u_1â‹…d_1+u_2â‹…d_2 &= u_1â‹…(d_1^{â€²}+d^{â€²â€²})+u_2â‹…d_2 \\
&= u_1â‹…d_1^{â€²}+u_1â‹…d^{â€²â€²}+u_2â‹…d_2 \\
&â‰¤ u_1â‹…d_1^{â€²}+u_2â‹…d^{â€²â€²}+u_2â‹…d_2 \\
&= u_1â‹…d_1^{â€²}+u_2â‹…(d_2+d^{â€²â€²}) \\
&= u_1â‹…d_1^{â€²}+u_2â‹…d_2^{â€²},
\end{aligned}$$

where $d_1=d_1^{â€²}+d^{â€²â€²}$ and $d_2=d_2^{â€²}-d^{â€²â€²}.$


### All Cross-assignments
We denote ordering with vector of indices such as $I^{â€²}=(1,2,...,k)$ for ordering $u_1â‰¤u_2â‰¤...â‰¤u_k.$

We can construct the set of all minimizing difference vectors by using cross-assignment over all possible orderings of vector $ğ®.$ We can generate all possible ordering of $ğ®$ by generating all permutations of $I.$ Let $\mathcal{P}(I)$ define the set of all permutations of set $I.$

$$Î”_ğ©=\{ğ^{âˆ—}(ğ®)âˆ£âˆ€ğ®\}=\{ğ^{âˆ—}(I^{â€²})âˆ£I^{â€²}âˆˆ\mathcal{P}(I)\}.$$

There are a finite amount of permutations $|\mathcal{P}(I)|=k!$ which implies $|Î”_ğ©|â‰¤k!.$

### Extrema
Special case where $Ïµâ‰¤-d_i^{-}$ and $Ïµâ‰¤d_i^{+}$ for all $iâˆˆ\{1,...,k\}.$
